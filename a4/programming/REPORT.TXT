CSC D84 - Artificial Intelligence, Winter 2015

Assignment 4 - Reinforcement Learning - Q Learning

This assignment is worth:

15 AIUs (Artificial Intelligence Units)
toward the 35% assignment component of your final
mark.

________________________________________________

Student Name (last, first): Dujay, Nicholas

Student number: 999194900

UTORid: dujaynic

READ THIS AND SIGN YOUR NAME AT THE END:

 I certify that I have read the UTSC code on academic
honesty and plaguarism. All work submitted as part
of this assignment is my own.

        Signed: _Nicholas Dujay__


(-5 marks for failing to provide the identifying
 information requested above)
________________________________________________

Answer the following questions. Be concise and clear
but explain carefully when needed.

1 .- (2 marks) Explain your reward function. What game elements
      are used, and why you think this is a good reward function
      for this problem.

I use the distance of the closest cheese and the closest cat. I subtract the
closest_cheese distance from the closest_cat because it gives you higher numbers
if cats are further away than the cheese, and negative rewards if the cat is
closer than the cheese. This encourages the mouse to take actions that take it
closer to the cheese every time.

2 .- (3 marks) These are multiple experiments (once you are sure your
     QLearning code is working!)

     * IMPORTANT! * For each training run, SAVE the Qtable
     you will re-use the Qtables later.

     # Experiment 1, 10000 training rounds
     initGame(1522,1)
     doTrain(10000,20)
     # SAVE YOUR Q TABLE!
     doGame()

     Run at least 50 rounds of the game.

     Record the mouse winning rate: 0.352941176471

     # Experiment 2
     initGame(1522,1)
     doTrain(1000000,50)
     # SAVE YOUR Q TABLE!    <--- You have to submit this one as 'Qtable.pickle'
     doGame()

     Run at least 50 rounds of the game.

     Record the mouse winning rate: 0.933333333333 (56/60)

     Would the mouse keep improving and become invincible if
     you ran, say, 100,000,000 training rounds per batch?

     There will be some situations where the mouse can't make the best decision
     if only due to cornering itself. Qlearning may never be able to learn not
     to corner itself if the mouse doesn't go to each spot where it can
     be cornered often enough.

4 .- (3 marks)

     Using the QTable for the training session with 1,000,000
     training trials:

     Record the mouse's winning rate for the following setups
     after 50 rounds of the game
     (NOTE: NO TRAINING THIS TIME AROUND. Re-use your Qtable)

     # 1
     initGame(4289,1)
     doGame()

     Mouse Winning Rate: 0.529411764706

     # 2
     initGame(31415,1)
     doGame()

     Mouse Winning Rate: 0.5

     # 3
     initGame(3210,1)
     doGame()

     Mouse Winning Rate: 0.566666666667

     Average winning rate: 0.5266666666667

     Explain the above numbers compared to the winning rates in 3),
     and provide some insight as to what is going on.

     The mouse has associated states with the specific maze, rather than a maze
     in general. When the mouse goes to a different maze, a specific move may
     have been bad for it on the 1522 seed's maze, but for the new maze it may
     have been a great move. Qlearning doesn't generalize to different mazes
     very well.

5 .- (2 marks) Is standard Q-Learning a rasonable strategy for environments
     that change constantly? discuss based on the above

     Based on the above, standard Q-Learning is not very good for constantly
     changing environments. The mouse associates rewards on the result from the
     environment rather than directly from the action in different situations.

7 .- (5 marks) Explain your feature set for feature-based Q Learning, and
               explain why and how each feature is expected to contribute
               to helping your mouse win

1. closest_cheese - this will help the mouse go towards cheese
2. closest_cat - this will help the mouse avoid cats
3. num_cats_nearby - this will help the mouse avoid many cats
4. checkForCheese(mouse) - again will help the mouse go towards cheese
5. checkForCats(mouse) - help the mouse avoid death
6. Ncats - give the mouse how dangerous it is
7. Ncheese - give the mouse an indication of its options

8 .- (5 marks) Carry out the following experiments:

     Experiment 0: (Baseline)

     Train the mouse as follows:
     initGame(15,2,2,3)
     doQLearn(10,1)

     this is to establish a baseline performance without actually doing much training

     run the game once the training is done

     doGame()

     Record the mouse winning rate after 50 rounds:

     Experiment 1: 0.0666666666667

     Train the mouse as follows:
     initGame(15,2,2,3)
     doQLearn(2500,10)     # <---- You are free to  use as many trials as you need so your mouse learns to win as much as possible
                           #       given your features

     * SAVE YOUR WEIGHTS FILE *  <---- You'll need to submit this file

     Then, start a new game:

     initGame(31415,2,2,3)
     doGame()

     Report mouse winning rate after 50 rounds: 0.612403100775

     Experiment 2:

     initGame(31415,3,2,3)
     doGame()

     Report mouse winning rate after 50 rounds: 0.55737704918

9 .- (5 marks) Based on the above, is feature-based learning better at
     dealing with changing environments than standard Q-learning?
     Provide a convincing (but short!) argument!

Based on my particular results, no. However, since features are independent of
the environment, feature-based learning will train the mouse to learn how to make
better moves rather than learn how to traverse the environment.

After observing the feature-based learning mouse versus the qlearning mouse, I
have noticed that the qlearning mouse will use the environment based on previous
safest paths where as the feature-based learning mouse will actively avoid cats
(but run into walls constantly), which suggests that the feature-based learning
mouse learns moves independent of the environment.
_____________________________________________________

Mark with an 'x' where appropriate. If something is only
working partially, briefly describe what works, what
doesn't work, or what problems exist.

                        Complete/Working        Partial         Not done

QLearn                         X
 update

Reward                         X
 function

Decide                         X
 action

featureEval                    X

evaluateQsa                    X

maxQsa_prime                   X

Qlearn_features                X

decideAction_features          X

_____________________________________________________

Marking:

(5 marks) Implemented QLearn update.

(5 marks) Implemented a reasonable reward function

(5 marks)  Implemented the function that chooses the
           optimal action based on Qtable and
           maze

(15 marks) Implemented a non-trivial, good feature set
           and evaluation function

(15 marks) Implemented a working feature-based Q-learning
           algorithm

(20 marks) Competitive mouse performance

(25 marks) Answers in this report file

(- marks)  Penalty marks

Total for A4:       / out of 90


