CSC D84 - Artificial Intelligence, Winter 2015

Assignment 5 - Neural Networks for OCR

This assignment is worth:

10 AIUs (Artificial Intelligence Units)
toward the 35% assignment component of your final
mark.

________________________________________________

Student Name (last, first): Dujay, Nicholas

Student number: 999194900

UTORid: dujaynic

READ THIS AND SIGN YOUR NAME AT THE END:

 I certify that I have read the UTSC code on academic
honesty and plagiarism. All work submitted as part
of this assignment is my own.

	Signed: _Nicholas Dujay__


(-5 marks for failing to provide the identifying
 information requested above)
________________________________________________

Answer the following questions. Be concise and clear
but explain carefully when needed.

Note: Empirically, I have had success with the following parameters:

    Learning rates in [2,6], threshold in [.01, .05] for 1-layer
     networks

    Learninng rates in [.25,10], threshold in [.01, .05] for 2-layer
     networks

    For 2-layer networks, I have seen good results with 15-30 hidden
     units

    You will have to try and see what works best for each network
     architecture.

1 .- (2.5 marks) Train a 1-layer network using the Logistic activation
               function.

		Report the learning rate used: 	4.5
		Report the threshold used:		0.03

		Train the network with 3 different random seeds,
		for each seed report:

		List of seeds used:
			4674
			6054
			9244

		Average classification accuracy on training set:
			75.9089720115
			81.3758828145
			81.3758828145

		Average classification accuracy on testing set:
			74.6800222593
			79.1875347802
			79.1875347802

		Save a screenshot of the trained network, call your
		file:

		 	1layer-­logistic-­final.jpg

2 .- (2.5 marks) Repeat the process above but using a 1-layer network
		with hyperbolic tangent activation function.

		Report the learning rate used:	3.0
		Report the threshold used:		0.05

		For the 3 random seeds report:

		List of seeds used:
			3491
			8234
			5123

		Average classification accuracy on training set:
			90.8187287471
			93.1205859273
			93.277530735

		Average classification accuracy on testing set:
			87.0339454647
			90.2058987201
			90.8180300501

		Save a screenshot of the trained network, call your
		file:

			1layer-tanh-final.jpg

3 .- (2.5 marks) Repeat the process above using a 2-layer network
		with hyperbolic tangent activation function.

		Report the learning rate used: 0.5
		Report the threshold used: 0.05
		Number of hidden units: 20

		For the 3 random seeds report:

		List of seeds used:
			25589
			8234
			5123

		Average classification accuracy on training set:
			89.8770599006
			92.5974365681
			94.8469788125

		Average classification accuracy on testing set:
			86.2548692265
			90.3728436283
			92.0422927101

		Save a screenshot of the trained network, call your
		file:

			2layer-tanh-final.jpg

4 .- (2.5 marks) Repeat the process above using a 2-layer network
		with logistic activation function.

		Report the learning rate used: 0.4
		Report the threshold used: 0.05
		Number of hidden units: 15

		For the 3 random seeds report:

		List of seeds used:
			4674
			6054
			9244

		Average classification accuracy on training set:
			9.83520795187
			10.1752550353
			9.99215275961

		Average classification accuracy on testing set:
			10.1279910963
			10.1279910963
			10.0166944908

		Save a screenshot of the trained network, call your
		file:

			2layer-logistic-final.jpg

5 .- (5 marks) Which network architecture (number of layers/type of
	       activation function) performs best, and why?

		Here you want to discuss in terms of classification
		accuracy, but also consider training time, ease to
		set the network parameters (learning rate and threshold,
		number of hidden units if applicable), and whether
		the network seems very sensitive to the parameters
		(e.g. unless the parameters are set to very specific
		values the network won't learn).

1layer hyperbolic tangent gives the best results for all the factors in play.
2layer hyperbolic tangent gives higher accuracy but is more sensitive to the
learning rate. If the learning rate is too high, it converges to a very high
squared error too early. The number of hidden units also seems to affect the
convergence of the hyperbolic tangent. The learning time also takes a while.

The logistic function seems to activate too early, and is less 'fluid' than hyperbolic tangent.

6 .- (10 marks) Describe how you would build a network to play the cat
	        and mouse game (move the mouse to help it survive).

		- Describe what the input is in terms of a vector of
		  values
		- Describe what the output layer looks like (how many
		  neurons and what they encode)
		- Describe the error function
		- How many layers should you use?

The input would be the positions of every agent, and each important goal. So,
in the case of the cat and mouse, it would be the positions of the cats, mouse,
and the cheese.

The output layer would be the goodness of each move, and the mouse would pick the
best move.

The error function could be the same as back propogation's error function.

2 hidden layers would probably be the best. The first hidden layer would learn the
features, like distance to cat, distance to cheese, number of cats nearby, etc.
The second layer would help decide when to move based on these features. For example,
if there are a group of cats around the cheese you are going to want to skirt
around the cheese rather than go straight towards it.

_____________________________________________________

Mark with an 'x' where appropriate. If something is only
working partially, briefly describe what works, what
doesn't work, or what problems exist.

        	Complete/Working	Partial		Not done

Logistic
 activation			X

Hyperbolic
 tangent
  activation 		X

sigmoid_prime()		X

Feed-Forward		X
 computation

Training by			X
 back-propagation
_____________________________________________________

Marking:

(5 marks) Sigmoid and Hyperbolic Tangent activation functions

(5 marks) Sigmoid prime function

(10 marks) Feed-forward computation

(20 marks) Backpropagation weight adjustments

(25 marks) Answers in this report

Total for A5:       / out of 65

